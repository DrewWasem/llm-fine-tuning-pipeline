base_model: meta-llama/Meta-Llama-3-8B-Instruct
torch_dtype: bfloat16
attn_implementation: flash_attention_2
gradient_checkpointing: true
load_in_4bit: true
