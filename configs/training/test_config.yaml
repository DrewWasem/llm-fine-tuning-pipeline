# Minimal training config for testing (no GPU required)
stage: lora

model:
  base_model: gpt2
  torch_dtype: float32
  attn_implementation: eager
  gradient_checkpointing: false
  load_in_4bit: false

data:
  max_length: 512
  packing: false

optimizer:
  learning_rate: 5e-5
  warmup_ratio: 0.1

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.0
  target_modules: ["c_attn"]

training:
  num_epochs: 1
  batch_size: 2
  gradient_accumulation_steps: 1
  max_steps: 10
  save_steps: 100
  logging_steps: 1

output_dir: outputs/test
wandb_project: ""
